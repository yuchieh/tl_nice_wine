{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- Data Path -----\n",
    "processed_data_path = 'features/'\n",
    "\n",
    "#----- Model Path -----\n",
    "model_path = 'models/'\n",
    "if not os.path.isdir(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    print(\"made folder:\", model_path)\n",
    "\n",
    "model_name = 'baseline_model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book = pd.read_csv(processed_data_path+'book.csv')\n",
    "df_trade = pd.read_csv(processed_data_path+'trade.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Feature Setting ===== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Price Features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'nextMidpt'\n",
    "\n",
    "bid_price_features = ['Bid1', 'Bid2', 'Bid3', 'Bid4', 'Bid5'] \n",
    "# ------ use bid_price features\n",
    "bid_price = True\n",
    "# bid_price = False\n",
    "\n",
    "ask_price_features = ['Ask1', 'Ask2', 'Ask3', 'Ask4', 'Ask5']\n",
    "# ------ use ask_price features\n",
    "ask_price = True\n",
    "# ask_price = False\n",
    "\n",
    "price_stats_features = ['MicroPrice', 'Bid_Mean', 'Ask_Mean']\n",
    "# ------ use price_stats_features\n",
    "price_stats = True\n",
    "price_stats = False\n",
    "\n",
    "speard_features = ['Spread1', 'Spread2', 'Spread3', 'Spread4', 'Spread5', 'SpreadMean']\n",
    "# speard_features = ['Spread1']\n",
    "# ------ use speard_features\n",
    "speard = True\n",
    "speard = False\n",
    "\n",
    "# ------ use midpt \n",
    "midpt = True\n",
    "# midpt = False\n",
    "\n",
    "price_features = []\n",
    "if bid_price:\n",
    "    price_features = price_features + bid_price_features\n",
    "if ask_price:\n",
    "    price_features = price_features + ask_price_features\n",
    "if price_stats:\n",
    "    price_features = price_features + price_stats_features\n",
    "if speard:\n",
    "    price_features = price_features + speard_features\n",
    "if midpt:\n",
    "    price_features.append('midpt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- size_features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "bid_size_features = ['Bid1SizeProp', 'Bid2SizeProp', 'Bid3SizeProp', 'Bid4SizeProp', 'Bid5SizeProp']\n",
    "# ------ use bid_size_features\n",
    "bid_size = True\n",
    "# bid_size = False\n",
    "\n",
    "ask_size_features = ['Ask1SizeProp', 'Ask2SizeProp', 'Ask3SizeProp', 'Ask4SizeProp', 'Ask5SizeProp']\n",
    "# ------ use ask_size_features\n",
    "ask_size = True\n",
    "# ask_size = False\n",
    "\n",
    "q_imb_features = ['Q_ImB1', 'Q_ImB2', 'Q_ImB3', 'Q_ImB4', 'Q_ImB5']\n",
    "# q_imb_features = ['Q_ImB1']\n",
    "# ------ use q_imb_features\n",
    "q_imb = True\n",
    "q_imb = False\n",
    "\n",
    "ba_ratio_features = ['BidAskRatio1', 'BidAskRatio2', 'BidAskRatio3', 'BidAskRatio4', 'BidAskRatio5', 'BidAskRatioTotal']\n",
    "# ------ use ba_ratio_features\n",
    "ba_ratio = True\n",
    "ba_ratio = False\n",
    "\n",
    "\n",
    "size_features = [] \n",
    "if bid_size: \n",
    "    size_features = size_features + bid_size_features\n",
    "if ask_size:\n",
    "    size_features = size_features + ask_size_features \n",
    "if q_imb:\n",
    "    size_features = size_features + q_imb_features \n",
    "if ba_ratio:\n",
    "    size_features = size_features + ba_ratio_features \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Feature Engineering ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- feature engineering setting -----\n",
    "\n",
    "# features to compute moving average\n",
    "MA_list = []\n",
    "# MA_list = MA_list + price_features\n",
    "win_size = [5, 10, 20]\n",
    "\n",
    "# features to normalization\n",
    "Norm_list = []\n",
    "# Norm_list = Norm_list + price_features + size_features\n",
    "# set scalar, default is MinMax\n",
    "Standard = False\n",
    "\n",
    "# features to compute percentage change\n",
    "PC_list = []\n",
    "# PC_list = PC_list + price_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Moving Average Features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing moving average of ', MA_list)\n",
    "\n",
    "if MA_list != []:\n",
    "    for feat in MA_list:\n",
    "        for i in win_size:\n",
    "            df_book[feat] = df_book[feat].rolling(i).mean() \n",
    "#             df_book[feat + '_MA' + str(i)] = df_book[feat].rolling(i).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Percentage Change Features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Computing Percentage Change of ', PC_list)\n",
    "\n",
    "if PC_list != []:\n",
    "    for feat in PC_list:\n",
    "        df_book[feat] = df_book[feat].pct_change()\n",
    "#         df_book[feat + '_Pct'] = df_book[feat].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check null, na, nan and drop rows with NaN values after feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null, na, nan \n",
    "# print(df_book.columns.values)\n",
    "# print('df_book.isnull().sum()', df_book.isnull().sum().values)\n",
    "# print('df_book.isnull().sum()', df_book.isnull().sum().values)\n",
    "# print('df_book.isnan().sum()', df_book.isna().sum().sum())\n",
    "# print(\"df_book.isna().sum()\", df_book.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('use_inf_as_na', True)\n",
    "df_book.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===== Split Data into Training and Validation ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----convert timestamp to date and seperate the training/validation set\n",
    "df_book['DateTime'] = pd.to_datetime(df_book['TimeStamp'], unit='us')\n",
    "df_trade['DateTime'] = pd.to_datetime(df_trade['TimeStamp'], unit='us')\n",
    "\n",
    "df_book['Date'] = df_book['DateTime'].apply(lambda x: x.date())\n",
    "pd.unique(df_book['Date'])[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df_val again for generalization test if have more time\n",
    "\n",
    "df_train = df_book[df_book['DateTime']<='2020-04-16 07:00']\n",
    "df_val = df_book[df_book['DateTime']>'2020-04-16 07:00']\n",
    "# df_book_t = df_book[df_book['Date']>'2020-04-10 06:56']\n",
    "# df_book_t[df_book_t['Date']>'2020-04-13 01:31']\n",
    "df_train\n",
    "df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "print('Normalizing ', Norm_list)\n",
    "if Norm_list != []:\n",
    "    if Standard:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "    # generate scaler with only training set for generalization\n",
    "    scaler.fit(df_train[Norm_list])\n",
    "\n",
    "\n",
    "    # normalization with scaler\n",
    "    print(df_train.head())\n",
    "    df_train[Norm_list] = scaler.transform(df_train[Norm_list])       \n",
    "    print(df_train.head())\n",
    "\n",
    "    print(df_val.head())\n",
    "    df_val[Norm_list] = scaler.transform(df_val[Norm_list])\n",
    "    print(df_val.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = price_features + size_features\n",
    "# feature_list = price_features\n",
    "\n",
    "print('Total number of features', len(feature_list))\n",
    "print('feature_list: ', feature_list)\n",
    "# shuffle data\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "x_train = df_train[feature_list].values\n",
    "y_train = df_train[target].values\n",
    "\n",
    "# shuffle data\n",
    "# df_val = df_val.sample(frac=1).reset_index(drop=True)\n",
    "x_val = df_val[feature_list].values\n",
    "y_val = df_val[target].values\n",
    "\n",
    "\n",
    "print(\"Shape of x, y train/val {} {} {} {}\".format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----- Baseline model -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "# from tensorflow.keras.utils import Model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "params = {'batch_size': 128, 'epochs': 300, 'lr': 0.0005, 'optimizer': 'adam'}\n",
    "\n",
    "dim = x_train.shape[1]\n",
    "\n",
    "input_x = Input(shape=(dim,))\n",
    "\n",
    "pred_y = Dense(1, activation='linear')(input_x)\n",
    "\n",
    "model = Model(inputs=input_x, outputs=pred_y)\n",
    "\n",
    "if params[\"optimizer\"] == 'rmsprop':\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "if params[\"optimizer\"] == 'sgd':\n",
    "    optimizer = optimizers.SGD(lr=params[\"lr\"], decay=1e-6, momentum=0.9, nesterov=True)\n",
    "if params[\"optimizer\"] == 'adam':\n",
    "    optimizer = optimizers.Adam(learning_rate=params[\"lr\"], beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "if params[\"optimizer\"] == 'nesterov':\n",
    "    optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=params[\"optimizer\"], metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, \n",
    "expand_nested=True,dpi=96,)\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.epoch = 0\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []        \n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        if self.epoch % 10 == 0:\n",
    "            print(\"epoch: {0} - train loss: {1:8.6f} - val loss: {2:8.6f}\".format(\n",
    "                self.epoch, \n",
    "                logs.get('loss'),\n",
    "                logs.get('val_loss'),                \n",
    "            ))\n",
    "        self.model.reset_states()\n",
    "        self.epoch += 1\n",
    "\n",
    "best_model_path = os.path.join(model_path, model_name)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=100, min_delta=0.0001)\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'log_training_batch.log'), append=True)\n",
    "rlp = ReduceLROnPlateau(monitor='val_loss', factor=0.02, patience=20, verbose=1, mode='min',\n",
    "                        min_delta=0.001, cooldown=1, min_lr=0.0001)\n",
    "mcp = ModelCheckpoint(best_model_path, monitor='val_loss', verbose=1,\n",
    "                      save_best_only=True, save_weights_only=False, mode='min', period=1) \n",
    "\n",
    "history = LossHistory(model=model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train, epochs=params['epochs'], verbose=1,\n",
    "                            batch_size=params['batch_size'], shuffle=True,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[history, es, rlp, mcp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.plot(history.train_losses)\n",
    "plt.plot(history.val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'val_loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate predictions and metrics'''\n",
    "\n",
    "x_val = df_val[feature_list].values\n",
    "\n",
    "pred = model.predict(x_val)\n",
    "pred = np.squeeze(pred)\n",
    "print(y_val.shape, pred.shape)\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval = model.evaluate(x_train, y_train, verbose=0)\n",
    "val_eval = model.evaluate(x_val, y_val, verbose=0)\n",
    "\n",
    "print(' ')\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - MSE: {:.4f}, RMSE: {:.4f}'.format(train_eval[0], train_eval[1]))\n",
    "print('Validation Data - MSE: {:.4f}, RMSE: {:.4f}'.format(val_eval[0], val_eval[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.plot(y_val[46000:])\n",
    "plt.plot(pred[46000:])\n",
    "plt.title('600519 nextMidpt prediction')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Tick')\n",
    "plt.legend(['Real nextMidpt','Predicted nextMidpt'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
